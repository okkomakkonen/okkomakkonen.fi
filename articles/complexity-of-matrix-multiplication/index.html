<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complexity of Matrix Multiplication: The Power of Recursion</title>
    <meta name="description"
        content="This is a written form of a talk in the ANTA seminar. The complexity of matrix multiplication represents the number of operations needed to compute a matrix product in the asymptotic limit. The first advance in asymptotic complexity was made in 1969 when Strassen introduced an algorithm that is capable of computing the product of two N × N matrices with O(N^{2.81}) operations, which is better than the naive algorithm that takes O(N^3) operations. The current record is an algorithm that is able to compute the product using just O(N^{2.372}) operations. We present the basic tools that are used to analyze this problem and present an algorithm that is even better than the Strassen algorithm. This includes studying the matrix multiplication tensor, its rank, and the so-called border rank.">

    <!-- Load font -->
    <link rel="stylesheet" href="https://brick.freetls.fastly.net/Roboto:300">

    <!-- Load stylesheet -->
    <link rel="stylesheet" href="/style.css">

    <!-- Load MathJax -->
    <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Generated with https://realfavicongenerator.net/ -->
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/icons/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
</head>

<body>
    <header>
        <h1>
            Complexity of Matrix Multiplication: The Power of Recursion
        </h1>
        <strong>
            December 12, 2022
        </strong>
        <noscript>
            <p class="no-javascript">
                Please enable JavaScript to show math notation.
            </p>
        </noscript>
        <p class="abstract">
            <strong>Abstract.</strong>
            This is a written form of a talk in the ANTA seminar.
            The complexity of matrix multiplication represents the number of
            operations needed to compute a matrix product in the asymptotic limit.
            The first advance in asymptotic complexity was made in 1969 when
            Strassen introduced an algorithm that is capable of computing the
            product of two <span class="math inline">\(N \times N\)</span> matrices
            with <span class="math inline">\(\mathcal{O}(N^{2.81})\)</span>
            operations, which is better than the naive algorithm that takes <span
                class="math inline">\(\mathcal{O}(N^3)\)</span> operations. The current
            record is an algorithm that is able to compute the product using just
            <span class="math inline">\(\mathcal{O}(N^{2.372})\)</span> operations.
            We present the basic tools that are used to analyze this problem and
            present an algorithm that is even better than the Strassen algorithm.
            This includes studying the matrix multiplication tensor, its rank, and
            the so-called border rank.
        </p>
        <p>
            Read the pdf version <a href="main.pdf">here</a>.
        </p>
    </header>
    <!-- Generated by pandoc -->
    <h2 id="introduction-and-strassens-algorithm">Introduction and
        Strassen’s algorithm</h2>
    <p>This note follows some great surveys on the topic: <span class="citation" data-cites="pan1984how">(Pan
            1984)</span>, <span class="citation" data-cites="stothers2010complexity">(Stothers
            2010)</span>, and <span class="citation" data-cites="blaeser2013fast">(Bläser 2013)</span>.</p>
    <p>We wish to compute the product of two <span class="math inline">\(N
            \times N\)</span> matrices <span class="math inline">\(A \in K^{N \times
            N}\)</span> and <span class="math inline">\(B \in K^{N \times
            N}\)</span> over some field <span class="math inline">\(K\)</span>. The
        field will not be specified at this stage and some of the results could
        even be generalized to arbitrary rings. Matrix multiplication is defined
        by <span class="math display">\[\begin{aligned}
            (AB)_{ik} = \sum_{j=1}^N A_{ij} B_{jk}.
            \end{aligned}\]</span> The obvious way to compute this requires <span class="math inline">\(N\)</span>
        multiplications and <span class="math inline">\(N - 1\)</span> additions for each term. As there
        are <span class="math inline">\(N^2\)</span> terms in the output, we
        need <span class="math inline">\(N^3\)</span> multiplications and <span class="math inline">\(N^3 - N^2\)</span>
        additions. In total, there are
        <span class="math inline">\(2N^3 - N^2 \in \mathcal{O}(N^3)\)</span>
        operations. For the simple case of <span class="math inline">\(N =
            2\)</span>, the naive algorithm requires <span class="math inline">\(8\)</span> multiplications and <span
            class="math inline">\(4\)</span> additions, which is a total of <span class="math inline">\(12\)</span>
        operations.
    </p>
    <p>In <span class="citation" data-cites="strassen1969gaussian">(Strassen
            1969)</span> Strassen introduced the following algorithm for computing
        the product of two <span class="math inline">\(2 \times 2\)</span>
        matrices. Compute the following <span class="math inline">\(7\)</span>
        products: <span class="math display">\[\begin{aligned}
            \mathrm{I} &amp;= (A_{11} + A_{22})(B_{11} + B_{22}), &amp;
            \mathrm{V} &amp;= (A_{11} + A_{12})B_{22}, \\
            \mathrm{II} &amp;= (A_{21} + A_{22})B_{11}, &amp; \mathrm{VI} &amp;=
            (A_{21} - A_{11})(B_{11} + B_{12}), \\
            \mathrm{III} &amp;= A_{11}(B_{12}-B_{22}), &amp; \mathrm{VII} &amp;=
            (A_{12}- A_{22})(B_{21} + B_{22}), \\
            \mathrm{IV} &amp;= A_{22}(B_{21}-B_{11}).
            \end{aligned}\]</span> Then, <span class="math display">\[\begin{aligned}
            (AB)_{11} &amp;= \mathrm{I} + \mathrm{IV} - \mathrm{V} +
            \mathrm{VII}, &amp; (AB)_{12} &amp;= \mathrm{III} + \mathrm{V}, \\
            (AB)_{21} &amp;= \mathrm{II} + \mathrm{IV}, &amp; (AB)_{22} &amp;=
            \mathrm{I} + \mathrm{III} - \mathrm{II} + \mathrm{VI}.
            \end{aligned}\]</span> This algorithm requires <span class="math inline">\(18\)</span> additions and <span
            class="math inline">\(7\)</span> multiplications, which is more
        operations than the naive algorithm for <span class="math inline">\(2
            \times 2\)</span> matrices. There have been some improvements, which
        have brought the number of additions down a little bit, but all require
        more operations than the naive algorithm.</p>
    <p>Notice that Strassen’s algorithm works over any (noncommutative)
        ring, so we can apply it recursively. If <span class="math inline">\(N =
            2^k\)</span>, then we can compute the product of two <span class="math inline">\(N \times N\)</span>
        matrices with 18 additions and
        7 multiplications over <span class="math inline">\(2^{k-1} \times
            2^{k-1}\)</span> matrices. These products can be computed by again
        applying the Strassen’s algorithm. Let us denote by <span class="math inline">\(A_k\)</span> the number of
        additions and by <span class="math inline">\(M_k\)</span> the number of multiplications. Then,
        <span class="math inline">\(M_k = 7^k\)</span> and <span class="math inline">\(A_k = 7A_{k-1} + 18\cdot
            4^{k-1}\)</span> and
        <span class="math inline">\(A_0 = 0\)</span>. By setting <span class="math inline">\(B_k = A_k - 7A_{k-1} =
            18\cdot 4^{k-1}\)</span>.
        Thus, <span class="math display">\[\begin{aligned}
            A_k &amp;= B_k + 7A_{k-1} = B_k + 7(B_{k-1} + 7A_{k-2}) = \dots =
            \sum_{i=0}^{k-1} 7^i B_{k-i} = 6 \cdot (7^k - 4^k).
            \end{aligned}\]</span> In total, we get <span class="math inline">\(6
            \cdot (7^k - 4^k)\)</span> additions and <span class="math inline">\(7^k\)</span> multiplications. As <span
            class="math inline">\(7^k = N^{\log 7 / \log 2}\)</span> we get that the
        multiplication can be done using just <span class="math inline">\(\mathcal{O}(N^{\log 7 / \log 2})\)</span>
        operations if <span class="math inline">\(N\)</span> is a power of two.
        If <span class="math inline">\(N\)</span> is not a power of two, we can
        round <span class="math inline">\(N\)</span> up to the nearest power of
        two, say <span class="math inline">\(2^k\)</span>. We can compute the
        <span class="math inline">\(N \times N\)</span> product by computing the
        <span class="math inline">\(2^k \times 2^k\)</span> product by padding
        with zeros. As <span class="math inline">\(2^k \leq 2N\)</span>, we get
        that the product can be computed using <span class="math inline">\(\mathcal{O}(N^{\log 7 / \log 2})\)</span>
        operations. As <span class="math inline">\(\log 7 / \log 2 \approx 2.807
            &lt; 3\)</span>, we have now found an asymptotically faster algorithm
        for computing the matrix product.
    </p>
    <p>Notice in the above recursive definition that <strong>what matters
            for the complexity is the number of multiplications</strong> in the
        Strassen’s algorithm, not the number of additions. This is because in
        the first step, the additions cost <span class="math inline">\(4^{k-1}\)</span> operations, but the
        multiplications cost at least <span class="math inline">\(7^k\)</span>
        operations. Therefore, we will be interested in minimizing the number of
        multiplications we do. Even scalar multiplications will be free compared
        to matrix multiplications. In <span class="citation" data-cites="winograd1971multiplication">(Winograd
            1971)</span> it was
        shown that the Strassen algorithm is optimal for multiplying <span class="math inline">\(2 \times 2\)</span>
        matrices.</p>
    <div id="def:omega_1" class="definition">
        <p><strong>Definition 1</strong> (Matrix multiplication exponent). The
            matrix multiplication exponent is defined as <span class="math display">\[\begin{aligned}
                \omega = \inf \{ \beta \mid \text{two $N \times N$ matrices can be multiplied using
                $\mathcal{O}(N^\beta)$ operations} \}.
                \end{aligned}\]</span></p>
    </div>
    <p>Notice that the matrix multiplication exponent is defined as the
        infimum, which means that it is not necessarily the case that the
        product can be computed using <span class="math inline">\(\mathcal{O}(N^\omega)\)</span> operations.
        However, the product can be computed using <span class="math inline">\(\mathcal{O}(N^{\omega +
            \varepsilon})\)</span>
        operations for all <span class="math inline">\(\varepsilon &gt;
            0\)</span>. Here we are only interested in computing the product of
        square matrices. A similar exponent can be defined also for nonsquare
        matrices. It still turns out that the products of nonsquare matrices is
        interesting for this note.</p>
    <p>The naive algorithm gives us the trivial upper bound of <span class="math inline">\(\omega \leq 3\)</span>. On
        the other hand,
        Strassen’s algorithm gives us the better bound <span class="math inline">\(\omega \leq \log 7 / \log 2\)</span>.
        The trivial
        lower bound of <span class="math inline">\(\omega\)</span> is given by
        the fact that the output contains <span class="math inline">\(N^2\)</span> entries, so <span
            class="math inline">\(\omega \geq 2\)</span>. There have been some lower
        bounds (<span class="citation" data-cites="blaeser1999lower">(Bläser
            1999)</span>), but none give <span class="math inline">\(\omega &gt;
            2\)</span>. It is an open problem whether <span class="math inline">\(\omega = 2\)</span> or <span
            class="math inline">\(\omega &gt; 2\)</span>.</p>
    <p>If we have an algorithm for computing the product of two <span class="math inline">\(n \times n\)</span> matrices
        for some fixed <span class="math inline">\(n &gt; 1\)</span>, which uses <span class="math inline">\(r\)</span>
        multiplications, then we can
        recursively apply this algorithm for matrices of size <span class="math inline">\(N = n^k\)</span>. This gives
        us a total of <span class="math inline">\(r^k\)</span> multiplications. Therefore, we can
        give an upper bound on the matrix multiplication exponent.</p>
    <div id="thm:power_of_recursion_1" class="theorem">
        <p><strong>Theorem 1</strong> (The power of recursion). <em>If we have
                an algorithm for computing the product of two <span class="math inline">\(n \times n\)</span> matrices
                for <span class="math inline">\(n &gt; 1\)</span>, which uses at most <span
                    class="math inline">\(r\)</span> multiplications, then we have <span
                    class="math display">\[\begin{aligned}
                    \omega \leq \log r / \log n.
                    \end{aligned}\]</span></em></p>
    </div>
    <p>The idea of the above theorem is that <strong>if you have an
            algorithm for multiplying small square matrices, then <span class="math inline">\(\omega\)</span> is
            determined by the number of
            multiplications in that algorithm</strong>. Strassen’s algorithm is an
        example of this, since the base algorithm uses <span class="math inline">\(r = 7\)</span> multiplications to
        compute the
        product of matrices of order <span class="math inline">\(n =
            2\)</span>.</p>
    <h2 id="the-matrix-multiplication-tensor-and-its-rank">The matrix
        multiplication tensor and its rank</h2>
    <p>Let <span class="math inline">\(U, V, W\)</span> be vector spaces and
        <span class="math inline">\(\varphi \colon U \times V \to W\)</span> a
        bilinear map. Then <span class="math display">\[\begin{aligned}
            \varphi(u, v) = \varphi(\sum_{i} u_i x_i, \sum_{j} v_j y_j) =
            \sum_{i}\sum_{j} u_i v_j \varphi(x_i, y_j),
            \end{aligned}\]</span> where <span class="math inline">\(\{x_i\}_i\)</span> and <span
            class="math inline">\(\{y_j\}_j\)</span> are bases for <span class="math inline">\(U\)</span> and <span
            class="math inline">\(V\)</span>. This means that <span class="math inline">\(\varphi\)</span> is determined
        by its action on
        the basis elements, <em>i.e.</em>, <span class="math inline">\(w_{ij} =
            \varphi(x_i, y_j)\)</span> determine <span class="math inline">\(\varphi\)</span> uniquely. On the other
        hand,
        <span class="math inline">\(f_i \colon u \mapsto u_i\)</span> and <span class="math inline">\(g_j \colon v
            \mapsto v_j\)</span> are linear
        functionals on <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. Therefore,
        <span class="math inline">\(\varphi\)</span> is uniquely associated with a
        tensor in the space <span class="math display">\[\begin{aligned}
            U^* \otimes V^* \otimes W.
            \end{aligned}\]</span> In fact, we have an isomorphism that sends the
        map <span class="math inline">\((u, v) \mapsto f(u)g(v)w\)</span> to the
        simple tensor <span class="math inline">\(f \otimes g \otimes
            w\)</span>. In general, if <span class="math inline">\(\varphi_1 \colon
            U_1 \times V_1 \to W_1\)</span> and <span class="math inline">\(\varphi_2 \colon U_2 \times V_2 \to
            W_2\)</span>
        are bilinear, then <span class="math display">\[\begin{aligned}
            \varphi \colon (U_1 \oplus U_2) \times (V_1 \oplus V_2) &amp;\to W_1
            \oplus W_2 \\
            (u_1 \oplus u_2, v_1 \oplus v_2) &amp;\mapsto \varphi_1(u_1, v_1)
            \oplus \varphi_2(u_2, v_2)
            \end{aligned}\]</span> is bilinear. Thus, there is a correspondence
        between such bilinear maps and the tensors in <span class="math display">\[\begin{aligned}
            (U_1^* \oplus U_2^*) \otimes (V_1^* \oplus V_2^*) \otimes (W_1
            \oplus W_2).
            \end{aligned}\]</span> We also have the embeddings of <span class="math inline">\(U_1^* \otimes V_1^*
            \otimes W_1\)</span> and <span class="math inline">\(U_2^* \otimes V_2^* \otimes W_2\)</span> to this
        space by mapping the simple tensors <span class="math display">\[\begin{aligned}
            f_1 \otimes g_1 \otimes w_1 &amp;\mapsto (f_1 \oplus 0) \otimes (g_1
            \oplus 0) \otimes (w_1 \oplus 0) \\
            f_2 \otimes g_2 \otimes w_2 &amp;\mapsto (0 \oplus f_2) \otimes (0
            \oplus g_2) \otimes (0 \oplus w_2).
            \end{aligned}\]</span> For brevity, we will write <span class="math display">\[\begin{aligned}
            s \odot t = \underbrace{t \oplus \dots \oplus t}_\text{$s$ times}.
            \end{aligned}\]</span> Similarly, we may write <span class="math display">\[\begin{aligned}
            (U_1^* \otimes V_1^* \otimes W_1) \otimes (U_2^* \otimes V_2^*
            \otimes W_2) \cong (U_1^* \otimes U_2^*) \otimes (V_1^* \otimes V_2^*)
            \otimes (W_1 \otimes W_2),
            \end{aligned}\]</span> which allows us to define the tensor product of
        tensors.
    </p>
    <p>The rank of a tensor is defined in the usual way, <em>i.e.</em>, the
        rank of <span class="math inline">\(t \in U^* \otimes V^* \otimes
            W\)</span> is the smallest <span class="math inline">\(r\)</span> such
        that <span class="math display">\[\begin{aligned}
            t = \sum_{i=1}^r f_i \otimes g_i \otimes w_i
            \end{aligned}\]</span> for some <span class="math inline">\(f_i \in
            U^*\)</span>, <span class="math inline">\(g_i \in V^*\)</span> and <span class="math inline">\(w_i \in
            W\)</span>. We denote the rank of <span class="math inline">\(t\)</span> by <span
            class="math inline">\(R(t)\)</span>. We have the following properties of
        the rank, which can easily be proven by writing out the rank
        decomposition of <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>.</p>
    <div id="lem:rank_properties" class="lemma">
        <p><strong>Lemma 1</strong>. <em>If <span class="math inline">\(t,
                    t_1\)</span> and <span class="math inline">\(t_2\)</span> are tensors,
                then <span class="math display">\[\begin{aligned}
                    R(t_1 \oplus t_2) &amp;\leq R(t_1) + R(t_2) \\
                    R(s \odot t) &amp;\leq sR(t) \\
                    R(t_1 \otimes t_2) &amp;\leq R(t_1)R(t_2).
                    \end{aligned}\]</span></em></p>
    </div>
    <p>We can interpret the rank decomposition of the tensor as follows. The
        bilinear map <span class="math inline">\(\varphi\)</span> can be
        computed by computing the linear forms <span class="math inline">\(f_i(u)\)</span> and <span
            class="math inline">\(g_i(u)\)</span>, then computing their products
        <span class="math inline">\(f_i(u)g_i(u)\)</span> and finally computing
        the sum of <span class="math inline">\(f_i(u)g_i(u)w_i\)</span>. If we
        only count the number of “bilinear multiplications” (<em>i.e.</em>, not
        scalar multiplications), then we need <span class="math inline">\(R(t)\)</span> multiplications to compute <span
            class="math inline">\(\varphi(u, v)\)</span>, where <span class="math inline">\(t\)</span> is the tensor
        associated to <span class="math inline">\(\varphi\)</span>.
    </p>
    <p>We set <span class="math inline">\(U = K^{m \times n}\)</span>, <span class="math inline">\(V = K^{n \times
            p}\)</span> and <span class="math inline">\(W = K^{m \times p}\)</span> and consider the map
        <span class="math display">\[\begin{aligned}
            \varphi \colon K^{m \times n} \times K^{n \times p} &amp;\to K^{m
            \times p} \\
            (A, B) &amp;\mapsto AB.
            \end{aligned}\]</span> This is a bilinear map. The tensor associated to
        the matrix multiplication is called the matrix multiplication tensor,
        and we denote this tensor by <span class="math inline">\(\langle m, n, p
            \rangle\)</span>. Using the definition of matrix multiplication we can
        write the matrix multiplication tensor as <span class="math display">\[\begin{aligned}
            \langle m, n, p \rangle = \sum_{i=1}^m \sum_{j=1}^n \sum_{k=1}^p
            \pi_{ij} \otimes \tau_{jk} \otimes E_{ik},
            \end{aligned}\]</span> where <span class="math inline">\(\pi_{ij}(A) =
            A_{ij}\)</span>, <span class="math inline">\(\tau_{jk}(B) =
            B_{jk}\)</span> and <span class="math inline">\(E_{ik}\)</span> is the
        elementary matrix with a 1 in the position indexed by <span class="math inline">\((i, k)\)</span>.
    </p>
    <p>If <span class="math inline">\(R(\langle m, n, p \rangle) =
            r\)</span>, then we can compute the product of a <span class="math inline">\(m \times n\)</span> matrix with
        an <span class="math inline">\(n \times p\)</span> matrix using <span class="math inline">\(r\)</span>
        multiplications. However, this is not
        necessarily optimal, since we only consider the linear forms <span class="math inline">\(f_i, g_i\)</span> as
        depending on the entries of
        <span class="math inline">\(A\)</span> and the entries of <span class="math inline">\(B\)</span>. If we allowed
        mixed linear forms, then
        we could use fewer multiplications. In any case, we have the following
        inequalities regarding the number of multiplications and the rank of the
        tensor <span class="math display">\[\begin{aligned}
            \# \text{multiplications} \leq R(\langle m, n, p \rangle) \leq 2
            \cdot \# \text{multiplications}.
            \end{aligned}\]</span> There are some cases where the number of
        multiplications required is strictly less than the rank of the
        associated tensor. Asymptotically we should not see a difference, so we
        can define the matrix multiplication exponent using the rank. The
        following proposition states that the two definitions match.
    </p>
    <div id="prop:omega" class="proposition">
        <p><strong>Proposition 1</strong>. <em>The matrix multiplication
                exponent can be written using the rank as <span class="math display">\[\begin{aligned}
                    \omega = \inf \{ \beta \mid R(\langle N, N, N \rangle) \in
                    \mathcal{O}(N^\beta) \}.
                    \end{aligned}\]</span></em></p>
    </div>
    <p>Studying the rank is much more well-behaved, so this is a good
        choice. We can now look at the direct sum and tensor product of two such
        matrix multiplication tensors. Firstly, the direct sum of <span class="math inline">\(\langle m, n, p
            \rangle\)</span> and <span class="math inline">\(\langle m&#39;, n&#39;, p&#39; \rangle\)</span>
        corresponds to the bilinear map <span class="math display">\[\begin{aligned}
            (A, X), (B, Y) \mapsto (AB, XY).
            \end{aligned}\]</span> Second, the tensor product of <span class="math inline">\(\langle m, n, p
            \rangle\)</span> and <span class="math inline">\(\langle m&#39;, n&#39;, p&#39; \rangle\)</span>
        corresponds to the matrix multiplication tensor <span class="math inline">\(\langle mm&#39;, nn&#39;, pp&#39;
            \rangle\)</span>. This corresponds to the well-known property <span class="math inline">\((A \otimes X)(B
            \otimes Y) = AB \otimes
            XY\)</span>. We can write the properties of the rank in terms of the
        matrix multiplication tensor as follows.</p>
    <div id="lem:mat_mul_tensor_rank" class="lemma">
        <p><strong>Lemma 2</strong>. <em>We have the following properties for
                the matrix multiplication tensors <span class="math display">\[\begin{aligned}
                    &amp;\langle m, n, p \rangle \otimes \langle m&#39;, n&#39;, p&#39;
                    \rangle = \langle mm&#39;, nn&#39;, pp&#39; \rangle, \\
                    &amp;R(\langle m, n, p \rangle) = R(\langle n, m, p \rangle) = \dots
                    = R(\langle p, n, m \rangle) \\
                    &amp;R(\langle m, n, p \rangle) \leq R(\langle m&#39;, n&#39;,
                    p&#39; \rangle) \qquad \text{if } m \leq m&#39;, n \leq n&#39;, p \leq
                    p&#39;.
                    \end{aligned}\]</span></em></p>
    </div>
    <p>It is obvious that <span class="math inline">\(R(\langle m, n, p
            \rangle) = R(\langle p, n, m \rangle)\)</span>, since <span class="math inline">\(B^T A^T = (AB)^T\)</span>,
        <em>i.e.</em>, the
        product of a <span class="math inline">\(p \times n\)</span> and a <span class="math inline">\(n \times
            m\)</span> matrix can be computed by
        computing the product of a <span class="math inline">\(m \times
            n\)</span> matrix and a <span class="math inline">\(n \times p\)</span>
        matrix. Furthermore, by constructing a simple isomorphism, we may deduce
        that <span class="math inline">\(R(\langle m, n, p \rangle) = R(\langle
            p, m, n \rangle)\)</span>. Thus, <span class="math inline">\(R(\langle
            m, n, p \rangle)\)</span> is constant for all permutations of <span class="math inline">\(m, n, p\)</span>.
        The last one can be obtained by
        embedding the smaller matrix multiplication in the larger one by padding
        with zeros.
    </p>
    <p>These should be interpreted as:</p>
    <ul>
        <li>
            <p><strong>matrix product can be computed by
                    partitioning</strong>,</p>
        </li>
        <li>
            <p><strong>switching the dimensions does not affect the
                    complexity</strong>, and</p>
        </li>
        <li>
            <p><strong>computing a large matrix product is at least as difficult
                    as computing a small matrix product</strong>.</p>
        </li>
    </ul>
    <p>We can now prove a better version of Theorem <a href="#thm:power_of_recursion_1" data-reference-type="ref"
            data-reference="thm:power_of_recursion_1">1</a>.</p>
    <div id="thm:power_of_recursion_2" class="theorem">
        <p><strong>Theorem 2</strong> (The power of recursion). <em>If <span class="math inline">\(R(\langle m, n, p
                    \rangle) \leq r\)</span> for
                some <span class="math inline">\(m, n, p\)</span> such that <span class="math inline">\(mnp &gt;
                    1\)</span>, then <span class="math display">\[\begin{aligned}
                    \omega \leq 3 \log r / \log mnp.
                    \end{aligned}\]</span></em></p>
    </div>
    <div class="proof">
        <p><em>Proof.</em> Using the properties of the matrix multiplication
            tensor and its rank, we get <span class="math display">\[\begin{aligned}
                R(\langle mnp, mnp, mnp \rangle) &amp;= R(\langle m, n, p \rangle
                \otimes \langle p, m, n \rangle \otimes \langle n, p, m \rangle) \\
                &amp;\leq R(\langle m, n, p \rangle)R(\langle p, m, n
                \rangle)R(\langle n, p, m \rangle) \\
                &amp;= R(\langle m, n, p \rangle)^3 \\
                &amp;\leq r^3.
                \end{aligned}\]</span> Then we may use Theorem <a href="#thm:power_of_recursion_1"
                data-reference-type="ref" data-reference="thm:power_of_recursion_1">1</a> to deduce that <span
                class="math display">\[\begin{aligned}
                \omega \leq \log r^3 / \log mnp = 3 \log r / \log mnp.
                \end{aligned}\]</span> ◻</p>
    </div>
    <p>The idea of the above theorem is that <strong>if you have an
            algorithm for multiplying small matrices, then <span class="math inline">\(\omega\)</span> is determined by
            the number of
            multiplications in that algorithm, even if the matrices are not
            square</strong>. In particular, we get the following algorithm for
        multiplying square matrices of order <span class="math inline">\(mnp\)</span>. Partition the matrices first to
        blocks of size <span class="math inline">\(np \times mp\)</span> and
        <span class="math inline">\(mp \times mn\)</span>. The full product can
        be computed using <span class="math inline">\(r\)</span> multiplications
        of the blocks. Partition the blocks further to blocks of size <span class="math inline">\(p \times m\)</span>
        and <span class="math inline">\(m \times n\)</span>. These products can be
        computed using <span class="math inline">\(r\)</span> block products.
        The final block products can be computed with <span class="math inline">\(r\)</span> multiplications. Therefore,
        the total
        number of multiplications is at most <span class="math inline">\(r^3\)</span>.
    </p>
    <h2 id="disjoint-matrix-multiplication">Disjoint matrix
        multiplication</h2>
    <p>We will now focus on the problem of computing two matrix products
        <span class="math inline">\(AB\)</span> and <span class="math inline">\(XY\)</span> simultaneously. This problem
        is known
        as disjoint matrix multiplication, since we assume no dependencies
        between the entries of the different matrices. We start by noting the
        equalities <span class="math display">\[\begin{aligned}
            (a + x)(b + y) - xb - (a + x)y &amp;= ab \\
            (a + x)(b + y) - xb - a(b + y) &amp;= xy.
            \end{aligned}\]</span> Using this we may write <span class="math inline">\(a = A_{ij}\)</span>, <span
            class="math inline">\(b
            = B_{jk}\)</span>, <span class="math inline">\(x = X_{jk}\)</span>,
        <span class="math inline">\(y = Y_{ki}\)</span>. Then, <span class="math display">\[\begin{aligned}
            (AB)_{ik} &amp;= \sum_{j=1}^n (A_{ij} + X_{jk})(B_{jk} + Y_{jk}) -
            \sum_{j=1}^n X_{jk} B_{jk} - \bigg[\sum_{j=1}^n (A_{ij} + X_{jk}) \bigg]
            Y_{ki} \\
            (XY)_{ji} &amp;= \sum_{k=1}^p (A_{ij} + X_{jk})(B_{jk} + Y_{jk}) -
            \sum_{k=1}^p X_{jk} B_{jk} - A_{ij}\bigg[\sum_{k=1}^p (B_{jk} + Y_{ki})
            \bigg].
            \end{aligned}\]</span> We may compute these sums by computing the
        products <span class="math display">\[\begin{aligned}
            (A_{ij} + X_{jk})(B_{jk} + Y_{jk}) \qquad &amp;\text{for all $i, j,
            k$} \\
            X_{jk} B_{jk} \qquad &amp;\text{for all $j, k$} \\
            \bigg[\sum_{j=1}^n (A_{ij} + X_{jk}) \bigg] Y_{ki} \qquad
            &amp;\text{for all $k, i$} \\
            A_{ij}\bigg[\sum_{k=1}^p (B_{jk} + Y_{ki}) \bigg] \qquad
            &amp;\text{for all $i, j$}.
            \end{aligned}\]</span> This means that we need to compute a total of
        <span class="math inline">\(mnp + mn + mp + np\)</span> products to
        compute both <span class="math inline">\(AB\)</span> and <span class="math inline">\(XY\)</span>. This
        computation will give us a
        decomposition of the corresponding tensor <span class="math display">\[\begin{aligned}
            \langle m, n, p \rangle \oplus \langle n, p, m \rangle
            \end{aligned}\]</span> into a sum of <span class="math inline">\(mnp +
            mn + mp + np\)</span> simple tensors. As we are computing two equally
        hard products, we would wish that we can compute one of the problems
        using half the number of computations. In other words, we wish for the
        following property of the ranks <span class="math display">\[\begin{aligned}
            R(\langle m, n, p \rangle \oplus \langle n, p, m \rangle) =
            R(\langle m, n, p \rangle) + R(\langle n, p, m \rangle).
            \end{aligned}\]</span> However, we do not have a proof of such a
        property. The general version of this property is known as Strassen’s
        conjecture, which is still open. We would still like to utilize such an
        idea, but without requiring the proof of Strassen’s conjecture.
    </p>
    <div id="thm:power_of_recursion_3" class="theorem">
        <p><strong>Theorem 3</strong> (The power of recursion). <em>If <span class="math inline">\(R(s \odot \langle m,
                    n, p \rangle) \leq r\)</span>
                and <span class="math inline">\(r = qs\)</span> for some <span class="math inline">\(m, n, p\)</span>
                such that <span class="math inline">\(mnp &gt; 1\)</span>, then <span
                    class="math display">\[\begin{aligned}
                    \omega \leq 3 \log \frac{r}{s} / \log mnp.
                    \end{aligned}\]</span></em></p>
    </div>
    <p>We would want to show that <span class="math inline">\(R(\langle m,
            n, p \rangle) \leq q\)</span>, which would give us the result. However,
        it will be easier to show that <span class="math display">\[\begin{aligned}
            R(s \odot \langle m^h, n^h, p^h \rangle) \leq q^h s.
            \end{aligned}\]</span> In the end we will show that this gives us the
        result asymptotically as <span class="math inline">\(h \to
            \infty\)</span>.</p>
    <div class="proof">
        <p><em>Proof.</em> In the case <span class="math inline">\(h =
                1\)</span> we clearly have <span class="math display">\[\begin{aligned}
                R(s \odot \langle m^1, n^1, p^1 \rangle) \leq r = q^1 s.
                \end{aligned}\]</span> Let us proceed by induction on <span class="math inline">\(h\)</span>. We have
            that <span class="math display">\[\begin{aligned}
                R(s \odot \langle m^{h+1}, n^{h+1}, p^{h+1} \rangle) &amp;= R((s
                \odot \langle m, n, p \rangle) \otimes \langle m^h, n^h, p^h \rangle) \\
                &amp;\leq R(r \odot \langle m^h, n^h, p^h \rangle) \\
                &amp;= R(q \odot (s \odot \langle m^h, n^h, p^h \rangle)) \\
                &amp;\leq q R(s \odot \langle m^h, n^h, p^h \rangle) \\
                &amp;\leq q q^h s = q^{h+1} s.
                \end{aligned}\]</span> Thus, <span class="math display">\[\begin{aligned}
                R(\langle m^h, n^h, p^h \rangle) \leq R(s \odot \langle m^h, n^h,
                p^h \rangle) \leq q^h s.
                \end{aligned}\]</span> By Theorem <a href="#thm:power_of_recursion_2" data-reference-type="ref"
                data-reference="thm:power_of_recursion_2">2</a> we get that for all
            <span class="math inline">\(h\)</span> <span class="math display">\[\begin{aligned}
                \omega \leq 3 \log (q^h s) / \log (mnp)^h.
                \end{aligned}\]</span> As <span class="math inline">\(\omega\)</span> is
            defined as the infimum, we must also have that <span class="math inline">\(\omega\)</span> is less than the
            limit of the
            upper bounds. Therefore, <span class="math display">\[\begin{aligned}
                \omega \leq \lim_{h \to \infty} 3 \frac{h \log q + \log s}{h \log
                mnp} = \lim_{h \to \infty} 3 \frac{\log q}{\log mnp} + \frac{3 \log s}{h
                \log mnp} = 3 \log q / \log mnp.
                \end{aligned}\]</span> ◻
        </p>
    </div>
    <p>The idea of the above theorem is that <strong>if you have an
            algorithm for multiplying <span class="math inline">\(s\)</span> small
            matrix products, then <span class="math inline">\(\omega\)</span> is
            determined by the number of multiplications in that algorithm divided by
            the number of products</strong>. Actually, we do not even need <span class="math inline">\(s \mid r\)</span>
        by approximating appropriately.
        The bound is obtained by accumulating the accelerating power of the
        disjoint matrix multiplication task by using recursion. The cost of
        computing multiple products goes to zero as the sizes of the matrices
        grow. We are only computing unnecessary products in the first recursive
        step, since the rest require <span class="math inline">\(r\)</span>
        multiplications, which can be peformed by computing the <span class="math inline">\(q\)</span> many <span
            class="math inline">\(s\)</span>-fold products using the algorithm.</p>
    <p>From this result it should be clear that the algorithms obtained in
        this way are not useful in practice, since their accelerating power will
        become useful only in the limit. We have also not shown that the matrix
        multiplication can be computed in <span class="math inline">\(\mathcal{O}(N^\beta)\)</span> operations, where
        <span class="math inline">\(\beta\)</span> is the upper bound in the
        theorem. Our obtained upper bound is a limiting upper bound. If <span class="math inline">\(\omega &gt;
            0\)</span>, then <span class="math inline">\(\omega\)</span> will be limiting in this sense by
        using a similar argument as in the above theorem.
    </p>
    <p>We can now utilize our result that <span class="math display">\[\begin{aligned}
            R(2 \odot \langle n, n, n \rangle) \leq n^3 + 3n^2.
            \end{aligned}\]</span> Theorem <a href="#thm:power_of_recursion_3" data-reference-type="ref"
            data-reference="thm:power_of_recursion_3">3</a> immediately gives us
        that <span class="math display">\[\begin{aligned}
            \omega \leq \log \frac{n^3 + 3n^2}{2} / \log n.
            \end{aligned}\]</span> In particular, <span class="math inline">\(n =
            13\)</span> gives us <span class="math inline">\(\omega \leq \log 1352 /
            \log 13 \leq 2.811\)</span>. This is slightly worse than the Strassen’s
        bound. It seems that all the work we did was wasted, since we were not
        able to improve on Strassen’s bound. However, we are still missing one
        tool.</p>
    <h2 id="approximate-algorithms-and-the-border-rank">Approximate
        algorithms and the border rank</h2>
    <p>Let us now concentrate on the field <span class="math inline">\(K =
            \mathbb{R}\)</span>, but these methods will eventually work for any
        field. We start this section with the same equalities as in the previous
        sections, but we replace <span class="math inline">\(y\)</span> with
        <span class="math inline">\(\lambda y\)</span>. Thus, <span class="math display">\[\begin{aligned}
            (a + x)(b + \lambda y) - xb - \lambda (a + x)y &amp;= ab \\
            \lambda^{-1} \bigg[ (a + x)(b + \lambda y) - xb - a(b + \lambda y)
            \bigg] &amp;= xy.
            \end{aligned}\]</span> We can compute these products with <span class="math inline">\(4\)</span>
        multiplications just like before.
        However, <span class="math inline">\(\lambda (a + x)y \to 0\)</span> as
        <span class="math inline">\(\lambda \to 0\)</span>, so we can
        approximate the products using just <span class="math inline">\(3\)</span> multiplications by discarding the
        term
        <span class="math inline">\(\lambda (a + x)y\)</span>. Therefore, we can
        approximate the tensor <span class="math inline">\(\langle m, n, p
            \rangle \oplus \langle n, p, m \rangle\)</span> with a sequence of
        tensors of rank at most <span class="math inline">\(mnp + mn +
            np\)</span>. The following definition was first given in <span class="citation"
            data-cites="bini1979n2">(Bini et al. 1979)</span>.
    </p>
    <div class="definition">
        <p><strong>Definition 2</strong> (Border rank). Let <span class="math inline">\(d \in \mathbb{N}\)</span>. We
            define <span class="math inline">\(R_d(t)\)</span> to be the smallest such <span
                class="math inline">\(r\)</span> such that <span class="math display">\[\begin{aligned}
                \lambda^d t + \mathcal{O}(\lambda^{d+1}) = t_d,
                \end{aligned}\]</span> where <span class="math inline">\(t_d\)</span> is
            a tensor of rank at most <span class="math inline">\(r\)</span> over the
            ring <span class="math inline">\(K[\lambda]\)</span>. Then the border
            rank of <span class="math inline">\(t\)</span> is <span class="math inline">\(\underline{R}(t) = \min_d
                R_d(t)\)</span>.</p>
    </div>
    <p>Over <span class="math inline">\(\mathbb{R}\)</span> or <span class="math inline">\(\mathbb{C}\)</span> this can
        be thought of as
        obtaining <span class="math inline">\(t\)</span> as the limit of tensors
        with rank at most <span class="math inline">\(r\)</span>. For order 2
        tensors, <em>i.e.</em>, matrices, the border rank and the rank coincide,
        since the rank is semi-continuous. We have a few properties of the
        border rank.</p>
    <div id="lem:border_rank_properties" class="lemma">
        <p><strong>Lemma 3</strong>. <em>Let <span class="math inline">\(t, t_1,
                    t_2\)</span> be a tensors and <span class="math inline">\(d, d_1, d_2
                    \in \mathbb{N}\)</span>. Then <span class="math display">\[\begin{aligned}
                    R(t) = R_0(t) &amp;\geq R_1(t) \geq \dots \geq \underline{R}(t), \\
                    R_d(t_1 \oplus t_2) &amp;\leq R_d(t_1) + R_d(t_2), \\
                    R_d(s \odot t) &amp;\leq s R_d(t) \\
                    R_{d_1 + d_2}(t_1 \otimes t_2) &amp;\leq R_{d_1}(t_1)R_{d_2}(t_2).
                    \end{aligned}\]</span></em></p>
    </div>
    <p>These are clear by representing the tensors using the representation
        in the definition of <span class="math inline">\(R_d(t)\)</span>. Even
        though the border rank represents approximation of the tensor, we can
        still gain useful information about the rank of the tensor by the
        following lemma.</p>
    <div id="lem:rank_border_rank" class="lemma">
        <p><strong>Lemma 4</strong>. <em>Let <span class="math inline">\(t\)</span> be a tensor and <span
                    class="math inline">\(d \in \mathbb{N}\)</span>. Then <span class="math inline">\(R(t) \leq c_d
                    R_d(t)\)</span>, where <span class="math inline">\(c_d\)</span> is a polynomial in <span
                    class="math inline">\(d\)</span>.</em></p>
    </div>
    <div class="proof">
        <p><em>Proof.</em> Let <span class="math inline">\(r = R_d(t)\)</span>.
            Then we can write the approximation <span class="math display">\[\begin{aligned}
                \lambda^d t + \mathcal{O}(\lambda^{d+1}) &amp;= \sum_{i=1}^r f_i
                \otimes g_i \otimes w_i \\
                &amp;= \sum_{i=1}^r \left(\sum_{\alpha=0}^d \lambda^\alpha
                f_i^{(\alpha)} \right) \otimes \left(\sum_{\beta=0}^d \lambda^\beta
                g_i^{(\beta)} \right) \otimes \left(\sum_{\gamma=0}^d \lambda^\gamma
                w_i^{(\gamma)} \right) \\
                &amp;= \sum_{\alpha=0}^d \sum_{\beta=0}^d \sum_{\gamma=0}^d
                \lambda^{\alpha + \beta + \gamma} \sum_{i=1}^r f_i^{(\alpha)} \otimes
                g_i^{(\beta)} \otimes w_i^{(\gamma)}.
                \end{aligned}\]</span> By comparing coefficients we get that <span
                class="math display">\[\begin{aligned}
                t = \sum_{\stackrel{\alpha, \beta, \gamma}{\alpha + \beta + \gamma =
                d}} \sum_{i=1}^r f_i^{(\alpha)} \otimes g_i^{(\beta)} \otimes
                w_i^{(\gamma)}
                \end{aligned}\]</span> There are at most <span class="math inline">\((d+1)^2\)</span> ways to choose
            <span class="math inline">\(\alpha, \beta, \gamma\)</span> such that <span class="math inline">\(\alpha +
                \beta + \gamma = d\)</span>, since we may
            choose <span class="math inline">\(\alpha, \beta\)</span> freely, which
            fixes <span class="math inline">\(\gamma\)</span>. Therefore, <span class="math inline">\(R(t) \leq (d+1)^2
                r\)</span> ◻
        </p>
    </div>
    <p>The above lemma will not give us a good algorithm for computing a
        product if we have an approximate algorithm, since we are multiplying
        the rank by a potentially large number. We will remedy this fact by
        using the same trick that we did in Theorem <a href="#thm:power_of_recursion_3" data-reference-type="ref"
            data-reference="thm:power_of_recursion_3">3</a>, <em>i.e.</em>, we
        accumulate the accelerating power of the approximate algorithm.</p>
    <div id="thm:power_of_recursion_4" class="theorem">
        <p><strong>Theorem 4</strong> (The power of recursion). <em>If <span class="math inline">\(\underline{R}(s \odot
                    \langle m, n, p \rangle)
                    \leq r\)</span> and <span class="math inline">\(r = qs\)</span> for some
                <span class="math inline">\(m, n, p\)</span> such that <span class="math inline">\(mnp &gt; 1\)</span>,
                then <span class="math display">\[\begin{aligned}
                    \omega \leq 3 \log \frac{r}{s} / \log mnp.
                    \end{aligned}\]</span></em></p>
    </div>
    <div class="proof">
        <p><em>Proof.</em> Let <span class="math inline">\(d\)</span> be such
            that <span class="math inline">\(R_d(s \odot \langle m, n, p, \rangle)
                \leq r\)</span>. By doing the same argument as in the proof of
            Theorem <a href="#thm:power_of_recursion_3" data-reference-type="ref"
                data-reference="thm:power_of_recursion_3">3</a> we get that <span class="math display">\[\begin{aligned}
                R_{dh}(s \odot \langle m^h, n^h, p^h \rangle) \leq q^h s.
                \end{aligned}\]</span> Thus, we may use the relation between the rank
            and the border rank to obtain <span class="math display">\[\begin{aligned}
                R(s \odot \langle m^h, n^h, p^h) \leq (dh + 1)^2 R_{dh}(s \odot
                \langle m^h, n^h, p^h \rangle) = (dh + 1)^2 q^h s
                \end{aligned}\]</span> for large enough <span class="math inline">\(dh\)</span> by Lemma <a
                href="#lem:border_rank_properties" data-reference-type="ref"
                data-reference="lem:border_rank_properties">3</a>. Hence, by Theorem <a href="#thm:power_of_recursion_3"
                data-reference-type="ref" data-reference="thm:power_of_recursion_3">3</a> we get that <span
                class="math display">\[\begin{aligned}
                \omega \leq \lim_{h \to \infty} 3 \log \left( (dh + 1)^2 q^h s
                \right) / \log (mnp)^h = \frac{3\log q}{\log mnp} + \lim_{h \to \infty}
                \frac{3 \log (dh + 1)^2 s}{h \log mnp} = 3 \log q / \log mnp.
                \end{aligned}\]</span> The last limit comes from the fact that the
            logarithm of a polynomial is strictly sublinear. ◻</p>
    </div>
    <p>The idea of the above theorem is that <strong>if you have an
            algorithm for multiplying <span class="math inline">\(s\)</span> small
            matrix products, then <span class="math inline">\(\omega\)</span> is
            determined by the number of multiplications in that algorithm divided by
            the number of products, even if the algorithm can only approximate the
            products arbitrarily well</strong>.</p>
    <p>We can use the above theorem to see that <span class="math display">\[\begin{aligned}
            \omega \leq \log \frac{n^3 + 2n^2}{2} / \log n
            \end{aligned}\]</span> for all <span class="math inline">\(n &gt;
            1\)</span>. In particular, if we choose <span class="math inline">\(n =
            6\)</span>, then <span class="math inline">\(\omega \leq \log 144 / \log
            6 \leq 2.774 &lt; \log 7 / \log 2\)</span>. This means that we have
        produced an algorithm that is asymptotically faster than the Strassen
        algorithm.</p>
    <h2 id="conclusions">Conclusions</h2>
    <p>Using the tools we were able to come up with an algorithm that can
        show that <span class="math inline">\(\omega \leq 2.774\)</span>. The
        same techniques can be used to show that <span class="math inline">\(\omega \leq 2.67\)</span> <span
            class="citation" data-cites="pan1984how">(Pan 1984)</span>. Much more advanced methods
        can be utilized, but are too complicated for this short note. These
        include partial matrix multiplication, Schönhage’s <span class="math inline">\(\tau\)</span>-theorem, Strassen’s
        Laser method,
        Coppersmith and Winograd tensor, <em>etc</em>. The current record is at
        <span class="math inline">\(\omega &lt; 2.37188\)</span>, which was
        achieved recently in October 2022 <span class="citation" data-cites="duan2022faster">(Duan, Wu, and Zhou
            2022)</span>.
    </p>
    <p>There has also been research into explaining some of these algorithms
        in a more understandable way. In particular, explaining the construction
        of the Strassen algorithm has been studied in <span class="citation"
            data-cites="landsberg2008geometry">(Landsberg 2008)</span>, <span class="citation"
            data-cites="burichenko2014symmetries">(Burichenko
            2014)</span>, <span class="citation" data-cites="grochow2017designing">(Grochow and Moore 2017)</span>,
        <span class="citation" data-cites="ikenmeyer2019strassen">(Ikenmeyer and
            Lysikov 2019)</span> using secant varieties, Segre isomorphisms, unitary 2-designs, and by considering
        different bases.
    </p>
    <h2 id="references">References</h2>
    <div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
        <div id="ref-bini1979n2" class="csl-entry" role="doc-biblioentry">
            Bini, D, M Capovani, G Lotti, and F Romani. 1979. <span>“<span><span
                        class="math inline">\(O(n^{2.7799})\)</span></span> Complexity for
                Matrix Multiplication. <span>Strassen</span> Algorithm Is Not
                Optimal.”</span> <em>Inform. Process. Lett</em> 8 (5): 234–35.
        </div>
        <div id="ref-blaeser1999lower" class="csl-entry" role="doc-biblioentry">
            Bläser, Markus. 1999. <span>“A <span class="math inline">\(5/2n^2\)</span>-Lower Bound for the Rank of <span
                    class="math inline">\(n \times n\)</span>-Matrix Multiplication over
                Arbitrary Fields.”</span> In <em>40th Annual Symposium on Foundations of
                Computer Science (Cat. No. 99CB37039)</em>, 45–50. IEEE.
        </div>
        <div id="ref-blaeser2013fast" class="csl-entry" role="doc-biblioentry">
            ———. 2013. <span>“Fast Matrix Multiplication.”</span> <em>Theory of
                Computing</em>, 1–60.
        </div>
        <div id="ref-burichenko2014symmetries" class="csl-entry" role="doc-biblioentry">
            Burichenko, Vladimir P. 2014. <span>“On Symmetries of the
                <span>Strassen</span> Algorithm.”</span> <em>arXiv Preprint
                arXiv:1408.6273</em>.
        </div>
        <div id="ref-duan2022faster" class="csl-entry" role="doc-biblioentry">
            Duan, Ran, Hongxun Wu, and Renfei Zhou. 2022. <span>“Faster Matrix
                Multiplication via Asymmetric Hashing.”</span> <em>arXiv Preprint
                arXiv:2210.10173</em>.
        </div>
        <div id="ref-grochow2017designing" class="csl-entry" role="doc-biblioentry">
            Grochow, Joshua A, and Cristopher Moore. 2017. <span>“Designing <span class="nocase">Strassen’s</span>
                Algorithm.”</span> <em>arXiv Preprint
                arXiv:1708.09398</em>.
        </div>
        <div id="ref-ikenmeyer2019strassen" class="csl-entry" role="doc-biblioentry">
            Ikenmeyer, Christian, and Vladimir Lysikov. 2019. <span>“Strassen’s
                <span class="math inline">\(2 \times 2\)</span> Matrix Multiplication
                Algorithm: A Conceptual Perspective.”</span> <em>Annali Dell’ Università
                Di Ferrara</em> 65 (2): 241–48.
        </div>
        <div id="ref-landsberg2008geometry" class="csl-entry" role="doc-biblioentry">
            Landsberg, J. 2008. <span>“Geometry and the Complexity of Matrix
                Multiplication.”</span> <em>Bulletin of the American Mathematical
                Society</em> 45 (2): 247–84.
        </div>
        <div id="ref-pan1984how" class="csl-entry" role="doc-biblioentry">
            Pan, Victor. 1984. <span>“How Can We Speed up Matrix
                Multiplication?”</span> <em>SIAM Review</em> 26 (3): 393–415.
        </div>
        <div id="ref-stothers2010complexity" class="csl-entry" role="doc-biblioentry">
            Stothers, Andrew James. 2010. <span>“On the Complexity of Matrix
                Multiplication.”</span> PhD thesis, The University of Edinburgh.
        </div>
        <div id="ref-strassen1969gaussian" class="csl-entry" role="doc-biblioentry">
            Strassen, Volker. 1969. <span>“Gaussian Elimination Is Not
                Optimal.”</span> <em>Numerische Mathematik</em> 13 (4): 354–56.
        </div>
        <div id="ref-winograd1971multiplication" class="csl-entry" role="doc-biblioentry">
            Winograd, Shmuel. 1971. <span>“On Multiplication of <span class="math inline">\(2 \times 2\)</span>
                Matrices.”</span> <em>Linear
                Algebra and Its Applications</em> 4 (4): 381–88.
        </div>
    </div>


</body>

</html>